\documentclass[a4paper, 10pt]{article}
\usepackage[pdftex]{graphicx}
\usepackage{fullpage}
\usepackage{longtable}
\usepackage{lipsum}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{dsfont}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage{sidecap}
\usepackage{mdframed}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{url}
\usepackage{verbatim}
\usepackage{siunitx}
\usepackage{array}
\usepackage{capt-of}
\usepackage{booktabs}
\usepackage{savetrees}
\usepackage{xspace}
\usepackage{titlesec}
\usepackage{blindtext}
\sloppy

\setlength{\parskip}{1em}

\title{\Large Linear Regression and Resampling Methods \\	
		 \normalsize Project 1 FYS-STK4155}
	
\author{Grzegorz Dariusz Kajda}

\newcounter{rowno}

\begin{document}
\maketitle

\begin{abstract}
	The following paper presents research conducted for the methods of Linear Regression, resampling techniques used for assesment of the models, and their application to real terrain data. The study of the regression methods starts with the application of Ordinary Lest Squares regression to a dataset consisting of polynomials of x and y, and fitting of said polynomials to Franke function. The models performance is then measuered through the mean squared error (MSE) function and R2-score, before being evalueated through the application of resampling techniques. Stating with bootstrap resampling, we are able to analyze the MSE in terms of a decompostion to bias and variance. We then proceed with applying kFold cross-validation to assess the models performance by dividing our data into k datasets. This process is then repeated for two other variants of regression, namely Ridge and Lasso regressions.  

	The study then proceeds by applying resampling techniques of bootstrap and kFold cross-validation to evaluate our model of the OLS, and repeat the process for Ridge and Lasso regression afterwards. By applying bootstrap to all three models, we are able to visualize the bias-variance tradeoff, and thus study the mean squared error as a function of bias and variance. By compairing the results of each model, we find that Ridge regression gives rise to the best fit, with OLS coming in second, while Lasso struggles to converge, making it difficult to gauge its performance.

	Testing the models using topograhic data yeilded better results for Lasso regression, however the computational expenses associated with its coordinate decenta algorithm proved to be a major limiting factor fo rits performance. Thus Ridge regression once again proves to produce the best fit, with the OLS falling slightly behind.      
\end{abstract}
\linespread{2.5}
\tableofcontents

\linespread{2.5}
\section{Introduction}
In a world experiencing an abundance of data never seen before, the wish of predicting the unkown has never been stronger. While the ability to predict unseen data is a nontrivial task, it most certainly is possible, and with the use of adequate tools, one may take advantage of the underlaying patterns that much of the data surrounding us displays. Today, one of the most commonly used methods for uncovering relationships between variables is machine learning, a filed within artificial intelligence which has made astounishing progress in the field of learning from data since 2011. And although there still exist problems that machine algorithms may struggle with, we shall prove that for many tasks, simple methods such as regression analysis will suffice.  

In this research paper we will hence study three various methods of Linear regression known as the Ordinary Least Squares regression, Ridge regression and Lasso regression, and how this methods can be used to model relationships between variables. Our resarch will begin with the generetion of a small, noisy dataset, which will be fed to our models to fit a weighted sum of polynomials upto n-th order to the Franke Function. We will then measure the performance of our algorithms by computing the mean squared error (MSE) and R2-score, before we proceed with the application of resampling techniques called bootsrap and kFold cross valtidation to our models. The former enables the decomposition of the models error into bias, variance and noice, while the latter can be used for the estimation of the test error associated with a given method of statistical learning. Lastly, when our models have been evaluated using synthetic data, we will repeat this procedure to model topograhic data.  

Following the introduction, the report introduces the fundamental mathematical theory and methods used, results obtained from running the algorithms, and a discussion about the perforamnce of the models presented in this research.   
\linespread{2.5}

\section{Theory}

\subsection{Linear models}
As mentioned in the introduction, the aim of this project is to study methods of Linear regression, which are one of the best tools for building predictive models when we have measured data. Now, Linear regression can be described as a statistical approach to the explanation of a dependant variable \textbf{z} in terms of at least one independent, predictor variable \textbf{x}. This allows us to model a measured respone \textbf{z} as a function of a set of k variables \textbf{x} = \emph{$(x_0, x_1, x_2, ... , x_{k-1})^T$}: 

\begin{gather*}
	\textbf{z} = f(\textbf{x}) + \epsilon
\end{gather*}

Here $\epsilon$ is the error of our approximation that we wish to minimize for all data points. Now if no prior knowlegde in the form of a functional relationship is available to us, we assume the existance of a linear relationship between variables \textbf{z} and \textbf{x}, which gives rise to the analytical equations of linear regression allowing us to write the expression above as

\begin{gather*}
	\textbf{z} = \tilde{\textbf{z}} + \epsilon
\end{gather*}

where \textbf{$\tilde{z}$} describes the product of the k features \textbf{x} and k regression parameters $\beta = (\beta_0, \beta_1, \beta_2, ... , \beta_{k-1})$, $\textbf{$\tilde{z}$} = \textbf{x}\beta$, and is known as our prediction. The $\beta$ parameters are the unknown variables that we wish find through solving the equation of linear regression.  Now expanding will often find ourselves in situations where we want to approximate a set of n such response variables, $\textbf{z} = (z_0, z_1, z_2, ... , z_{n-1})$. One of the most common solutions to this problem is to parametrize our linear equation in terms of a polynomial function of n-1 degree: 


\begin{gather*}
	\textbf{z} = f(\textbf{x}) + \epsilon = \tilde{\textbf{z}} + \epsilon = \sum_{j=0}^{n-1} \beta_j x_{i}^j + \epsilon_i 
\end{gather*}

which as you shall see, is the approach used throughout this project. With a little linear algebra, we can stack all the feature vectors $\textbf{x}$ on top of each other to form a \emph{design matrix}


$$
\boldsymbol{X}=
\begin{bmatrix} 
1& x_{0}^1 &x_{0}^2& \dots & \dots &x_{0}^{k-1}\\
1& x_{1}^1 &x_{1}^2& \dots & \dots &x_{1}^{k-1}\\
1& x_{2}^1 &x_{2}^2& \dots & \dots &x_{2}^{k-1}\\                      
\dots& \dots &\dots& \dots & \dots &\dots\\
1& x_{n-1}^1 &x_{n-1}^2& \dots & \dots &x_{n-1}^{k-1}\\
\end{bmatrix}
$$

also known as the \emph{Vandermonde matrix}. Using the design matrix, and the set of k regression parameters \textbf{$\beta$}, our set of linear regression equations can be rewritten as
 
\begin{gather*}
	\tilde{y} = \textbf{X}\beta + \epsilon
\end{gather*}

With a model of general linear regression defined, and our goal of minimizing the error of the models approximation, we can now move on to the Least Squares regression. 

\subsubsection{Ordinary Least Squares}
In Ordinary Least Squared regression, we approach the task of finding the optimal parameters $\beta$ for our model defined in the section above, by defining a cost function describing the average squared difference between our predicted values \textbf{$\tilde{z}$} and the actual values \textbf{z}, namely: 

$$
C(\boldsymbol{\beta})=\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right\},
$$

which we rewrite to a more compact form with the use of the design matrix \textbf{X}

$$
C(\boldsymbol{\beta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)\right\}.
$$

Now, in order to compute the optimal parameters $\beta$, we are going to minimize the cost function by differentiating it with respect to the parameters $\beta$, and setting the resulting equation equal to zero. In other words, we will minimize the distance between the predicted data points, and the target values by solving the following problem

$$
\frac{\partial C(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = 0
$$

Which results in

$$
\frac{\partial C(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right).
$$

By applying the rules of matrix mutliplication, we can rewrite the resulting expression as follows

$$
\boldsymbol{X}^T\boldsymbol{y} = \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta},
$$

In the simple case where the matrix $X^TX$ is invertible, we can simply solve this equation by multiplying both sides from the left with the inverse of this matrix, $(X^TX)^(-1)$, giving us

$$
\boldsymbol{\beta} =\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
$$

\subsubsection{Ridge Regression}

\subsubsection{Lasso Regression}

\subsection{The Data}

\subsection{Preprocessing of Data}

\subsection{Singular Value Decomposition - SVD}

\subsection{Metrics for measurement of performance}

\subsection{Resampling techniques}

\subsubsection{The Bootstrap}


\section{Results}

\section{Conclussion and discussion of results}


\section{Appendix A - Analytical Solution of OLS}
Now let us assume that there exists a continous function \emph(f(x)) with a normally distributed error $\epsilon$ ~ \emph{N(0,$\sigma^2$)} which describes our data: 

\begin{gather*}
	\textbf{ $y=f(x) + \epsilon$}
\end{gather*}

Function f(x) has been approximated through our model \textbf{$\tilde{y}$}, where we minimized the \emph{Residuals sum of squares} \textbf{$(y-\tilde{y})^2$}, where: 

\begin{gather*}
	\textbf{ $\tilde{y}=X\beta$}
\end{gather*}
As we know, \textbf{X} is our design matrix containing all of the independent variables \textbf{x} used to approximate \textbf{y}. We are now going to show that the expectation value of \textbf{y} for any given element \emph{i} can be written in the following way: 

\begin{gather*}
	\mathbb{E}[y] = \Sigma_{j} x_{i} \beta_{j} = X_i,* \beta 
\end{gather*}

Let us start the proof with the element by rewriting the expactation value of \textbf{y}:

\begin{gather*}
	\mathbb{E}[y] = (1/n)*\Sigma_{j} y_{i} = (1/n)*\Sigma_{i=0} (f(x_i) + \epsilon_i) 
\end{gather*}

Now we see that in order to prove out that $\mathbb{E}[y]$ is equal to the product $X_{i,*}\beta$, we need to prove that the value of $\epsilon_{i} = 0$. We can easily do it by finding the first derivative of the cost functions MSE: 

\begin{gather*}
	\frac{\partial C(\beta)}{\partial \beta} = 0
\end{gather*}

As you can see, we set the derivative equal to zero in order to find the optimal parameters that will minimize our error. 

\begin{gather*}
	X^T(y-X\beta) = X^Ty - X^TX = 0
\end{gather*}

Now if this matrix $X^TX$ is invertible, which it is only if X is orthonormal, then with little algebra, we have the following solution for the optimal parameters: 

\begin{gather*}
	\beta = (X^TX)^{-1}X^Ty
\end{gather*}

Now in the situation where $X^TX$ is invertible, the error which we try to minimize will be equal to zero: 

\begin{gather*}
	\epsilon = y - \tilde{y} = y - X(X^TX)^{-1}X^Ty = y - y = 0 
\end{gather*}

If you pay attention however, we could've from the start assumed that the value of $\epsilon=0$, and written the proof in the following way: 

\begin{gather*}
	\mathbb{E}[y_i] = \mathbb{E}[X_i,*+beta] + \mathbb{E}[\epsilon_i] \\
						= X_i,*beta + 0 = \mathbb{E}[y] = X_i,*beta
\end{gather*}

This is simply caused by $\mathbb{E}[\epsilon_i]$ being by definition equal to zero, as it can be interpreted as the mean value of the error. Since the the mean value of the distribution of $\epsilon$ is equal to zero, we can write $\epsilon_i=0$. Now the next thing we are going to prove, is that the variance of $y_i$ is equal to $\sigma^2$. From the lecture notes and \emph{Pattern Recognition and Machine Learning by Christopher M. Bishop}, we know that the equation giving us variance, can be written in terms of a expectation value: 

\begin{gather*}
			Var(y_i) = \mathbb{E}[y_i - \mathbb{E}[y_i]] = \mathbb{E}[y_i^2] - (\mathbb{E}[y_i])^2 \\
										= \mathbb{E}[(X_i,*\beta + \epsilon_i)^2] - (X_i,*\beta)^2 \\
										= \mathbb{E}[(X_i,*\beta)^2 + 2\epsilon_iX_i,*\beta + \epsilon_i^2] - (X_i,*\beta)^2
										= (X_i,*\beta)^2 + 2\mathbb{E}
\end{gather*}

\end{document}


