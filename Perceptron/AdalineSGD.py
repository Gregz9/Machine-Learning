from numpy.random import seed
import numpy as np
import os 
import pandas as pd 
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap 


def plot_decision_regions(X, y, classifier, resolution=0.02): 

    # Configuring generator of markers and kolors 
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    # Drawing chart of decision area
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), 
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    

    # Draws the chart of examples 
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y ==cl, 0], y=X[y == cl, 1], 
                alpha=0.8, color=colors[idx], 
                marker=markers[idx], label = cl,
                edgecolor='black')

class AdalineSGD(object): 
    """Classification algorithm - ADAptive LInear NEuron.
    
    Parameters
    ----------
    eta: floating point variable
        Learning factor (values ranging from 0.0 to 1.0)
    n_iter: integer variable 
        Number of iterations through training dataset
    shuffle: boolean variable (default value set to 'True' )
        If value is set to True, shuffle every example in the training dataset 
        before a new era/iteration starts
    random_state: integer variable 
        Seed of the generator of pseudo-random numbers used to genarate weights
    
    Attributes
    ----------
    w_: One-dimensional table (Vector) 
        Wieghts to be fit 
    cost_: list
        Sum of squared errors/misclassification values from every training example in every era/iteration (value of the function of cost) 
    """

    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None): 
        self.eta = eta
        self.n_iter = n_iter 
        self.w_initialized = False 
        self.shuffle = shuffle
        self.random_state = random_state
    
    def fit(self, X, y): 
        """ fitting of training data 
        
        Parameters 
        ----------
        X: {table-alike}, dimensions = [n_examples, n_attributes] (Matrix) 
            learning vectors
        y: {table-alike}, dimensions = [n_examples]
            target values (answers to check if classification performed by the neuron is correct) 
        
        Output/Return value 
        -------------------
        self: object 
        """

        self._initialize_weights(X.shape[1])
        self.cost_ = []
        for i in range(self.n_iter): 
            if self.shuffle: 
                X, y = self._shuffle(X, y)
            cost = []
            for xi, target in zip(X, y): 
                cost.append(self._update_weights(xi, target))
            avg_cost = sum(cost) / len(y)
            self.cost_.append(avg_cost)
        return self 

    def partial_fit(self, X, y): 
        """Fitting training data without initializing weights over again"""
        if not self.w_initialized: 
            self._initialize_weights(X.shape[1])
        if y.ravel().shape[0] > 1: 
            for xi, target in zip(X, y): 
                self._update_weights(xi, target)
        else: 
            self._update_weights(X, y)
        return self 
    
    def _shuffle(self, X, y): 
        """Shuffle training data"""
        r = self.rgen.permutation(len(y))
        return X[r], y[r]

    def _initialize_weights(self, m): 
        """ Initialize weights by assigning small random values to each weight variable"""
        self.rgen = np.random.RandomState(self.random_state) # <= If the value of self.random_state == None, the set of numbers generated by the next method will be different for every time. If you however choose to pass an 
                                                             # integer as the argument of method RandomState, a certain set of pesudo-random numbers will be repeated each time we generate weights. 

        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m) # <= this method indigious to nthe NumPy library initializes weight-variables, each with a random value ranging from 0.0 to 1.0.
        self.w_initialized = True 

    def _update_weights(self, xi, target): 
        """ Uses the same learning principle as AdalineGD to update values of weights, 
        the only difference being the fact that we no longer compute the value to update the wieghts with 
        out of the sum of squared errors
        """
        output = self.activation(self.net_input(xi))
        error = (target - output)
        self.w_[1:] += self.eta * xi.dot(error)
        self.w_[0] += self.eta * error 
        cost = 0.5 * error**2 
        return cost
    
    def net_input(self, X): 
        """ Computes the total stimulation after taking in z as input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, X): 
        """ Computes the function of linear activation"""
        return X 

    def predict(self, X): 
        """ Returns the label of the class which the algorithm predicts. This predicted label may be wrong,
        that's why we have the y vector which holds the correct answers to each preditions, so that we can loop up 
        if the algorithm performs well."""
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)

if __name__ == "__main__": 
    
    s = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'

    print('URL Address: ', s)
    df = pd.read_csv(s, header=None, encoding='utf-8')
    print(df.tail())

    # We are choosing types of iris flowers 'setosa' and 'versicolor'
    y = df.iloc[0:100, 4].values 
    y = np.where(y == 'Iris-setosa', -1, 1)

    # length of sepal and flower flake length 
    X = df.iloc[0:100, [0, 2]].values

    # Standarized values of variables
    X_std = np.copy(X)
    X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()
    X_std[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()

    ada_sgd = AdalineSGD(n_iter=15, eta=0.01, random_state=1) 
    ada_sgd.fit(X_std, y)
    #ada_sgd.partial_fit(X_std[0, :], y[0])

    plot_decision_regions(X_std, y, classifier=ada_sgd)
    plt.title('Adaline - Stochastic decent along the gradient')
    plt.xlabel('Length of flower sepal [standarized]') # Standarized as in the statisical understanding 
    plt.ylabel('Length of flower petal [standarized]')
    plt.legend(loc='upper left')
    plt.tight_layout()
    plt.show()

    plt.plot(range(1, len(ada_sgd.cost_) + 1), ada_sgd.cost_, marker='o')
    plt.xlabel('Eras')
    plt.ylabel('Average cost')
    plt.tight_layout()
    plt.show()
